{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "verbose = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AT_T_TripletGenerator(Dataset):    \n",
    "    def __init__(self, dat_folder,train = True, transform = None,not_hold_out=True,hold_out_positive = False):   \n",
    "        super(AT_T_TripletGenerator, self).__init__()\n",
    "        self.root_dir = os.path.dirname(dat_folder) \n",
    "        self.transform = transform\n",
    "        \n",
    "        #list of folders \n",
    "        self.ind = [individual for individual in os.listdir(self.root_dir) if os.path.isdir(os.path.join(self.root_dir,individual))]\n",
    "        print(\"number of individuals {}\".format(len(self.ind)))\n",
    "        \n",
    "        if not_hold_out:\n",
    "            csv_file_train = os.path.join(self.root_dir, \"data_verification_Train.csv\")\n",
    "            csv_file_test = os.path.join(self.root_dir, \"data_verification_Test.csv\")\n",
    "        else:\n",
    "            csv_file_train = os.path.join(self.root_dir, \"data_verification_Train_hold_out.csv\")\n",
    "            csv_file_test = os.path.join(self.root_dir, \"data_verification_Test_hold_out.csv\")    \n",
    "        if hold_out_positive:\n",
    "            csv_file_train = os.path.join(self.root_dir, \"data_verification_Train_hop.csv\")\n",
    "            csv_file_test = os.path.join(self.root_dir, \"data_verification_Test_hop.csv\")    \n",
    "        print(\"filenames in use : {} Train, {} Test\".format(csv_file_train,csv_file_test))\n",
    "            \n",
    "        if (not os.path.basename(csv_file_train) in os.listdir(self.root_dir) or (not os.path.basename(csv_file_test) in os.listdir(self.root_dir))):\n",
    "        \n",
    "            train_line_list = []\n",
    "            test_line_list = [] \n",
    "\n",
    "            for i,individual in enumerate(self.ind):\n",
    "                folder_path = os.path.join(self.root_dir,individual)\n",
    "                name_images = [name for name in os.listdir(folder_path)]          \n",
    "                random.shuffle(name_images)\n",
    "\n",
    "                for j in range(len(name_images)-1):\n",
    "                    if os.path.isfile(os.path.join(folder_path,name_images[j])):\n",
    "                        anchor_filename = os.path.join(individual,name_images[j])\n",
    "                        if verbose: print(\"anchor {}{}\".format(j,anchor_filename))\n",
    "                        \n",
    "                        begin,end = 0, len(name_images)-1\n",
    "                        if not_hold_out and hold_out_positive:\n",
    "                            if j < ((len(name_images)-1)*0.8):\n",
    "                                begin,end = int(0), int(len(name_images)*0.8)-1 \n",
    "                            else:\n",
    "                                begin,end = int(len(name_images)*0.8)-1, int(len(name_images))\n",
    "                            \n",
    "                        positive_index = np.delete(np.arange(begin,end),np.argwhere(np.arange(begin,end)==j))                        \n",
    "                        if verbose: print(\"positive {}\".format(positive_index))\n",
    "                        \n",
    "                        negative_index_folder = random.sample(range(0,len(self.ind)-1),len(positive_index))                        \n",
    "                        if i in negative_index_folder:\n",
    "                            if i == 0: negative_index_folder[negative_index_folder.index(i)] = i+1\n",
    "                            else:negative_index_folder[negative_index_folder.index(i)] = i-1\n",
    "                        if verbose: print(\"negative folders {}\".format(negative_index_folder))\n",
    "                        \n",
    "                        negative_index = []\n",
    "                        for negative_folder in negative_index_folder:\n",
    "                            num_images = len(os.listdir(os.path.join(self.root_dir,self.ind[negative_folder])))\n",
    "                            negative_index.append(np.random.randint(num_images,size=1))\n",
    "                                                \n",
    "                        for bid in range(len(positive_index)):                   \n",
    "                            positive_filename = os.path.join(individual,name_images[positive_index[bid]])\n",
    "                            negative_folder = os.path.join(self.root_dir,self.ind[negative_index_folder[bid]])\n",
    "                            negative_filename = os.path.join(self.ind[negative_index_folder[bid]],os.listdir(negative_folder)[int(negative_index[bid])])\n",
    "                            \n",
    "                            if verbose: print(\"positive filename {}\".format(positive_filename))\n",
    "                            if verbose: print(\"negative filename {}\".format(negative_filename))\n",
    "                            if not_hold_out: \n",
    "                                split_iter = j\n",
    "                                split_enum = len(name_images)-1\n",
    "                            else: \n",
    "                                split_iter = i\n",
    "                                split_enum = len(self.ind)-1\n",
    "                            line = [anchor_filename,positive_filename,negative_filename]\n",
    "                            if split_iter < (split_enum*0.8):  train_line_list.append([line])\n",
    "                            else: test_line_list.append([line])\n",
    "\n",
    "            with open(csv_file_train,\"w\") as f:\n",
    "                writer = csv.writer(f,delimiter=',')\n",
    "                writer.writerow([\"Anchor\",\"Positive\", \"Negative\"])\n",
    "                for lines in train_line_list:\n",
    "                    writer.writerows(lines)\n",
    "\n",
    "            with open(csv_file_test,\"w\") as f:\n",
    "                writer = csv.writer(f,delimiter=',')\n",
    "                writer.writerow([\"Anchor\",\"Positive\", \"Negative\"])\n",
    "                for lines in test_line_list:\n",
    "                    writer.writerows(lines) \n",
    "    \n",
    "        if train: self.AT_T_datafile = pd.read_csv(csv_file_train)\n",
    "        else: self.AT_T_datafile = pd.read_csv(csv_file_test)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.AT_T_datafile)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        anchor = self.__loadfile(os.path.join(self.root_dir,self.AT_T_datafile.iloc[idx,0]))\n",
    "        positive = self.__loadfile(os.path.join(self.root_dir,self.AT_T_datafile.iloc[idx,1]))\n",
    "        negative = self.__loadfile(os.path.join(self.root_dir,self.AT_T_datafile.iloc[idx,2]))\n",
    "        \n",
    "        if self.transform:\n",
    "            anchor = Image.fromarray(anchor)\n",
    "            anchor = self.transform(anchor)\n",
    "            positive = Image.fromarray(positive)\n",
    "            positive = self.transform(positive)\n",
    "            negative = Image.fromarray(negative)\n",
    "            negative = self.transform(negative)\n",
    "        \n",
    "        return anchor,positive,negative\n",
    "    \n",
    "    def __loadfile(self, data_file):\n",
    "        image = io.imread(data_file)\n",
    "        if len(image.shape)<3:\n",
    "            image = np.stack((image,)*3, axis=-1)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder = \"/home/rita/JupyterProjects/EYE-SEA/DataSets/Verification/ATeT_faces/orl_faces/\"\n",
    "#dataset_transform = transforms.Compose([\n",
    "#   transforms.Resize((224,224)),\n",
    "#   transforms.ToTensor(),        \n",
    "#   transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "#])\n",
    "#generator = AT_T_TripletGenerator(folder, transform = dataset_transform,not_hold_out = True, hold_out_positive = True)\n",
    "#\n",
    "#dataloaders = torch.utils.data.DataLoader(generator,batch_size=4, shuffle=True)\n",
    "#inputs = next(iter(dataloaders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
