{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import csv\n",
    "from skimage import io\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "import pathlib\n",
    "import os\n",
    "import copy\n",
    "from datetime import date\n",
    "\n",
    "import import_ipynb\n",
    "import ResNetCaps_E\n",
    "import Dataset_Loader\n",
    "import losses\n",
    "import MiniBatch_generator\n",
    "\n",
    "verbose = False\n",
    "load_model = False\n",
    "LFW_use = False\n",
    "ATET_use = True\n",
    "folder = 'ATET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),        \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "if ATET_use : folderDataset = \"/home/rita/JupyterProjects/EYE-SEA/DataSets/Verification/ATeT_faces/orl_faces/\"\n",
    "if LFW_use : folderDataset = \"/home/rita/JupyterProjects/EYE-SEA/DataSets/Verification/lfw/\"\n",
    "#folderDataset = \"/media/Data/rita/EYE-SEA/Verification/Datasets/ATeT_faces/orl_faces/\"\n",
    "batch_size = 256\n",
    "\n",
    "Train_loader = Dataset_Loader.Folded_Dataset(folderDataset, dataset_transform,0.8)\n",
    "dataLoader_generator = torch.utils.data.DataLoader(Train_loader,batch_size=batch_size)\n",
    "Test_loader = Dataset_Loader.Folded_Dataset(folderDataset, dataset_transform,0.8,train = False)\n",
    "dataLoader_generator_test = torch.utils.data.DataLoader(Test_loader,batch_size=batch_size)\n",
    "\n",
    "dataset = MiniBatch_generator.mini_batch(folderDataset, dataset_transform,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH= os.path.join(os.getcwd(),os.path.join('Log_model/Cluster_Loss',folder,'DIGIT/',(date.today()).isoformat()))\n",
    "pathlib.Path(PATH).mkdir(parents=True, exist_ok=True)\n",
    "if len(os.listdir(PATH)) > 2 and load_model:\n",
    "    print('Loading model from PATH: {}'.format(PATH))\n",
    "    model = ResNetCaps_E.ResNetCaps_E(DigitEnd=False)\n",
    "    if pick_model == -1:\n",
    "        init = len(os.listdir(PATH))-2        \n",
    "        model.load_state_dict(torch.load(os.path.join(PATH,str(init))))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(os.path.join(PATH,str(pick_model))))\n",
    "        init = pick_model\n",
    "    model.eval()\n",
    "else:\n",
    "    print('Creating a new model')\n",
    "    init=0\n",
    "    model = ResNetCaps_E.ResNetCaps_E()  \n",
    "    \n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterLoss(nn.Module):\n",
    "    def __init__(self,alpha=0.2):\n",
    "        super(ClusterLoss,self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.ranking_loss = nn.SoftMarginLoss()\n",
    "        \n",
    "        self.clusters_sum = []\n",
    "        self.clusters_count = []\n",
    "        self.clusters_labels = []\n",
    "        \n",
    "    def forward(self,feats,targets):       \n",
    "       \n",
    "        t_intra,D_intra = self.Euclidean_intra(feats,targets)\n",
    "        t_inter,D_inter = self.Euclidean_inter(targets)\n",
    "        \n",
    "        Y = (torch.Tensor(t_intra).data.new().resize_as_(torch.Tensor(t_intra).data).fill_(1))\n",
    "        Y = Y.to(device)\n",
    "        loss = self.ranking_loss((D_intra-D_inter)+self.alpha,Y)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def mean_feats(self,feats,targets):\n",
    "        N = feats.size(0)\n",
    "        \n",
    "        # shape [N, N]\n",
    "        is_pos = targets.expand(N, N).eq(targets.expand(N, N).t())\n",
    "        is_neg = targets.expand(N, N).ne(targets.expand(N, N).t())\n",
    "        \n",
    "        target_batch = []\n",
    "        for i in range(N):\n",
    "            t = targets[i]\n",
    "            if not t in target_batch:\n",
    "                a = feats[is_pos[:,i],:]#list of features computed over the same individual\n",
    "                sum_a = torch.sum(a,dim=0)              \n",
    "\n",
    "                if self.clusters_sum:\n",
    "                    if self.clusters_labels:                       \n",
    "                        if t in self.clusters_labels:\n",
    "                            j = self.clusters_labels.index(t.item())\n",
    "                            self.clusters_sum[j] += sum_a\n",
    "                            self.clusters_count[j] += a.size(0)\n",
    "                        else:\n",
    "                            self.clusters_sum.append(sum_a)\n",
    "                            self.clusters_labels.append(t)\n",
    "                            self.clusters_count.append(a.size(0))\n",
    "                    else: \n",
    "                        print('There are no labels {}'.format(M_label))\n",
    "                else:\n",
    "\n",
    "                    self.clusters_sum.append(sum_a)\n",
    "                    self.clusters_labels.append(t.item())\n",
    "                    self.clusters_count.append(a.size(0))\n",
    "                target_batch.append(t)\n",
    "                \n",
    "    def mean_feats_compute(self):\n",
    "       \n",
    "        M_emb = [sum_a/len_a for sum_a,len_a in zip(self.clusters_sum,self.clusters_count)]\n",
    "        self.M_emb = torch.stack(M_emb).to(device)\n",
    "                \n",
    "    def Euclidean_intra(self,feats,targets):\n",
    "        \n",
    "        M_intra = self.M_emb\n",
    "        D = losses.euclidean_distance(feats,M_intra)\n",
    "        N = feats.size(0)\n",
    "                       \n",
    "        is_pos = targets.expand(N, N).eq(targets.expand(N, N).t())\n",
    "        \n",
    "        target_intra = []\n",
    "        D_intra = []\n",
    "        for i in range(N):          \n",
    "            if not targets[i].item() in target_intra:\n",
    "                D_id = D[is_pos[:,i],:]            \n",
    "                target_intra.append(targets[i].item())\n",
    "                index_mean = self.clusters_labels.index(targets[i].item())\n",
    "                D_intra.append(torch.max(D_id[:,index_mean]))\n",
    "        \n",
    "        D_intra = torch.stack(D_intra)\n",
    "        D_intra = D_intra.to(device)\n",
    "        return target_intra, D_intra\n",
    "    \n",
    "    def Euclidean_inter(self,targets):\n",
    "        M_intra = self.M_emb\n",
    "        N = targets.size(0)\n",
    "        is_neg = targets.expand(N, N).ne(targets.expand(N, N).t())\n",
    "        target_inter = []\n",
    "        D_inter= []\n",
    "        for i in range(N):\n",
    "            if not targets[i].item() in target_inter:\n",
    "                index_mean = self.clusters_labels.index(targets[i].item())\n",
    "                M = M_intra[index_mean,:]  \n",
    "                target_inter.append(targets[i].item())  \n",
    "                list_inter = []\n",
    "                for j in range(len(M_intra)):\n",
    "                    if not j == index_mean:\n",
    "                        X = (M_intra[j,:])\n",
    "                        list_inter.append(torch.pairwise_distance(M.unsqueeze(1),X.unsqueeze(1),2))\n",
    "                D_inter.append(torch.min(torch.stack(list_inter)))\n",
    "\n",
    "        \n",
    "        D_inter = torch.Tensor(D_inter)\n",
    "        D_inter = D_inter.to(device)\n",
    "        D_inter.requires_grad_()\n",
    "        return target_inter, D_inter\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = MiniBatch_generator_2.mini_batch(folderDataset, dataset_transform,0.8)\n",
    "criterion = ClusterLoss()\n",
    "#compute the mean embedding value\n",
    "with torch.no_grad():\n",
    "    for batch_id, (in_train, labels_train) in enumerate(dataLoader_generator):\n",
    "        #in_train = Variable(in_train)\n",
    "        in_train = in_train.to(device)    \n",
    "        emb_train = model(in_train)\n",
    "        criterion.mean_feats(emb_train.squeeze(),labels_train)\n",
    "   \n",
    "    criterion.mean_feats_compute()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "loss_list = []\n",
    "start = time.time()\n",
    "for epoch in range(n_epochs): \n",
    "    print('epoch {}:{}'.format(epoch+1, n_epochs)) \n",
    "    loss_list_b = []\n",
    "    #for batch_id, (in_a,labels)  in enumerate(dataLoader_generator):\n",
    "     #   int_a = Variable(in_a)\n",
    "     #   in_a = in_a.to(device)\n",
    "     #   labels = labels.to(device)\n",
    "    in_a,labels = dataset.prepare_batch()\n",
    "    in_a = torch.stack(in_a)\n",
    "    in_a = in_a.to(device)\n",
    "    labels = torch.Tensor(labels).to(device)\n",
    "    model.train()\n",
    "    emb_a = model(in_a)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(emb_a.squeeze(),labels)\n",
    "\n",
    "    #if batch_id%10==0:\n",
    "    print(\"loss per batch {}\".format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_list_b.append(loss)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for batch_id, (in_train, labels_train) in enumerate(dataLoader_generator):\n",
    "            #in_train = Variable(in_train)\n",
    "            in_train = in_train.to(device)    \n",
    "            emb_train = model(in_train)\n",
    "            criterion.mean_feats(emb_train.squeeze(),labels_train)   \n",
    "        criterion.mean_feats_compute()  \n",
    "        \n",
    "    loss_list.append(sum(loss_list_b)/batch_id)\n",
    "#torch.save(model.state_dict(), os.path.join(PATH,str(epoch)))\n",
    "end = time.time()\n",
    "print('Training time: {} s'.format(end - start))\n",
    "print('mean time per epoch: {} s'.format((end - start)/(n_epochs - 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold(var,percentile):\n",
    "    hist, bin_edges = np.histogram(var,100)\n",
    "    cdf = np.float32(np.cumsum(hist))/np.sum(hist)\n",
    "    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2\n",
    "    threshold = np.interp(percentile*0.01, cdf, bin_centers)    \n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = []\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        in_a,_ = dataset.prepare_batch()\n",
    "        in_a = torch.stack(in_a)\n",
    "        in_a = in_a.to(device)\n",
    "        emb_a = model(in_a)  \n",
    "        D = losses.euclidean_distance(emb_a.squeeze(),emb_a.squeeze()) #criterion.M_emb)\n",
    "        v = torch.zeros(D.size(0)).to(device).type(D.dtype)\n",
    "        mask = torch.diag(torch.ones_like(v)).to(device).type(D.dtype)\n",
    "        D_m = (mask * torch.diag(v) + (1. - mask) * D).cpu().detach().numpy()\n",
    "        D_m = D_m[~np.eye(D_m.shape[0],dtype=bool)].reshape(D_m.shape[0],D_m.shape[1] - 1)\n",
    "        thresholds.append(find_threshold(D_m,0.25))\n",
    "    print(np.max(thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1,n_epochs+1)\n",
    "plt.plot(epochs, loss_list, color='pink')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training phase')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=100000)\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%TEST%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "model.eval()\n",
    "in_a_test,labels_test = dataset.prepare_batch_test()\n",
    "in_a_test = torch.stack(in_a_test)\n",
    "in_a_test = in_a_test.to(device)\n",
    "labels_test = torch.Tensor(labels_test).to(device)\n",
    "emb_a = model(in_a_test)\n",
    "\n",
    "D = (losses.euclidean_distance(emb_a.squeeze(),emb_a.squeeze()))\n",
    "#D_m = D[~np.eye(D.shape[0],dtype=int)].reshape(D.shape[0],-1)\n",
    "v = torch.zeros(D.size(0)).to(device).type(D.dtype)\n",
    "mask = torch.diag(torch.ones_like(v)).to(device).type(D.dtype)\n",
    "D_m = (mask * torch.diag(v) + (1. - mask) * D).cpu().detach().numpy()\n",
    "b = (D_m < np.max(thresholds)).astype(int)\n",
    "\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_test)\n",
    "a=[]\n",
    "labels_t = labels_test.cpu().detach().numpy()\n",
    "for i in range(len(labels_t)):\n",
    "    a.append([np.append(labels_t[i:-1],labels_t[0:i])])\n",
    "a = np.vstack(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(D_m, cmap= cm.get_cmap('Blues', 20))\n",
    "cbar = fig.colorbar(im, ticks=[np.min(D_m), np.max(thresholds), np.max(D_m)])\n",
    "cbar.ax.set_yticklabels([np.min(D_m), np.max(thresholds), np.max(D_m)])\n",
    "plt.show()\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "im2 = ax2.imshow(a, cm.get_cmap('Blues', 7))\n",
    "cbar2 = fig2.colorbar(im2, ticks=[np.min(a), np.max(a)])\n",
    "cbar2.ax.set_yticklabels([np.min(a), np.max(a)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(D,labels,th):\n",
    "   \n",
    "    N = D.size(0)\n",
    "    print(N)\n",
    "    # shape [N, N]\n",
    "    is_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\n",
    "    is_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\n",
    "    \n",
    "    # Exclude selfs for positive samples\n",
    "    device = labels.device\n",
    "    v = torch.zeros(N).to(device).type(is_pos.dtype)\n",
    "    mask = torch.diag(torch.ones_like(v)).to(device).type(is_pos.dtype)\n",
    "    is_pos = mask * torch.diag(v) + (1. - mask) * is_pos\n",
    "\n",
    "    # `dist_ap` means distance(anchor, positive)\n",
    "    dist_ap = D[is_pos].contiguous().view(N, -1)\n",
    "    # `dist_an` means distance(anchor, negative)\n",
    "    dist_an = D[is_neg].contiguous().view(N, -1)\n",
    "    \n",
    "    #threshold = torch.mean(dist_ap) + (torch.mean(dist_an) - torch.mean(dist_ap))/2\n",
    "    \n",
    "    positives_True =  0\n",
    "    for i in dist_ap:\n",
    "        for j in range(len(i)):\n",
    "            if i[j].item() < th: positives_True += 1 \n",
    "    negatives_True =  0\n",
    "    for i in dist_an:\n",
    "        for j in range(len(i)):\n",
    "            if i[j].item() > th: negatives_True += 1   \n",
    "    \n",
    "    VAL = positives_True/dist_ap.numel()\n",
    "    FAR = negatives_True/dist_an.numel()\n",
    "    \n",
    "    return positives_True, negatives_True, VAL, FAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_T, N_T, VAL, FAR = accuracy(D,labels_test,np.max(thresholds))\n",
    "print(\"Loss {}. Threshold {}: P_T {} N_T {} VAL {} FAR {}\".format(loss, np.max(thresholds), P_T, N_T, VAL, FAR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
