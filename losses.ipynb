{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import import_ipynb\n",
    "from ohem import hard_example_mining, hard_aware_point_2_set_mining\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x: pytorch Variable, with shape [m, d]\n",
    "      y: pytorch Variable, with shape [n, d]\n",
    "    Returns:\n",
    "      dist: pytorch Variable, with shape [m, n]\n",
    "    \"\"\"\n",
    "    m, n = x.size(0), y.size(0)\n",
    "    #sqrt((x-y)^2)\n",
    "    xx = torch.pow(x, 2).sum(1, keepdim=True).expand(m, n)\n",
    "    yy = torch.pow(y, 2).sum(1, keepdim=True).expand(n, m).t()\n",
    "    dist = xx + yy\n",
    "    dist.addmm_(1, -2, x, y.t())\n",
    "    return dist.clamp(min=1e-12).sqrt()  # for numerical stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold(var,percentile):\n",
    "    hist, bin_edges = np.histogram(var,100)\n",
    "    cdf = np.float32(np.cumsum(hist))/np.sum(hist)\n",
    "    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2\n",
    "    threshold = np.interp(percentile*0.01, cdf, bin_centers)    \n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpreadLoss(nn.Module):\n",
    "    def __init__(self, num_classes, m=0.5):\n",
    "        super(SpreadLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.m = m\n",
    "\n",
    "    def forward(self, x, target, **pars):  # x:b,10 target:b\n",
    "        m = self.m\n",
    "        if len(pars) > 0:\n",
    "            m = pars['extra_pars'][0]\n",
    "\n",
    "        one_shot_target = torch.eye(self.num_classes).index_select(dim=0, index=target.data.cpu()).to(x.device)\n",
    "        a_t = torch.sum(x * one_shot_target, dim=1)\n",
    "        zeros = torch.zeros(x.size()).to(x.device)\n",
    "        loss = torch.sum((torch.max(zeros, m - (a_t[:, None] - x))) ** 2, dim=1) - (m**2)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "        # a_t = torch.Tensor([x[i][target[i]] for i in range(b)])  # b\n",
    "        # a_t_stack = a_t.view(b, 1).expand(b, self.num_classes).contiguous().to(x.device) # b, num_classes\n",
    "        # u = m - (a_t_stack - x)  # b,10\n",
    "        # mask = u.ge(0).float()  # max(u,0) #b,10\n",
    "        # loss = ((mask * u) ** 2).sum() / b # NM => ???? - m ** 2  # float\n",
    "        # return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"Modified from Tong Xiao's open-reid (https://github.com/Cysu/open-reid).\n",
    "    Related Triplet Loss theory can be found in paper 'In Defense of the Triplet\n",
    "    Loss for Person Re-Identification'.\"\"\"\n",
    "    def __init__(self, margin=None, process_dists=False):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.process_dists = process_dists\n",
    "        if margin is not None:\n",
    "            self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
    "        else:\n",
    "            self.ranking_loss = nn.SoftMarginLoss()\n",
    "\n",
    "    def forward(self, x, y, z=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        dist_ap: pytorch Variable, distance between anchor and positive sample,\n",
    "        shape [N]\n",
    "        dist_an: pytorch Variable, distance between anchor and negative sample,\n",
    "        shape [N]\n",
    "        Returns:\n",
    "        loss: pytorch Variable, with shape [1]\n",
    "        \"\"\"\n",
    "        if not self.process_dists:\n",
    "            d_ap = F.pairwise_distance(x, y, 2)\n",
    "            d_an = F.pairwise_distance(x, z, 2)\n",
    "        else:\n",
    "            d_ap = x\n",
    "            d_an = y\n",
    "        Y = d_an.data.new().resize_as_(d_an.data).fill_(1)\n",
    "        if self.margin is not None:\n",
    "            loss = self.ranking_loss(d_an, d_ap, Y)\n",
    "        else:\n",
    "            loss = self.ranking_loss(d_an - d_ap, Y)\n",
    "        return loss, d_ap, d_an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnan(x):\n",
    "    return x != x\n",
    "\n",
    "#TripletLoss layer \n",
    "class TripletLossLayer(torch.nn.Module):\n",
    "    def __init__(self,alpha):\n",
    "        super(TripletLossLayer, self).__init__()\n",
    "        self.ALPHA = alpha\n",
    "        self.ranking_loss = nn.SoftMarginLoss()\n",
    "        \n",
    "    def triplet_loss(self,a,p,n):\n",
    "        \n",
    "        p_dist =  F.pairwise_distance(a, p, 2)\n",
    "        n_dist = F.pairwise_distance(a,n,2)\n",
    "        Y = p_dist.data.new().resize_as_(p_dist.data).fill_(1)\n",
    "        loss = self.ranking_loss(p_dist-n_dist+self.ALPHA,Y)\n",
    "        return [loss,p_dist,n_dist]\n",
    "    \n",
    "    def forward(self,a,p,n):\n",
    "        loss, p_dist, n_dist = self.triplet_loss(a,p,n)\n",
    "        self.loss = loss\n",
    "        return loss, p_dist, n_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardMiningTripletLoss(nn.Module):\n",
    "    \"\"\"Modified from Tong Xiao's open-reid (https://github.com/Cysu/open-reid).\n",
    "    Related Triplet Loss theory can be found in paper 'In Defense of the Triplet\n",
    "    Loss for Person Re-Identification'.\"\"\"\n",
    "\n",
    "    def __init__(self, margin=None):\n",
    "        super(HardMiningTripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        if margin is not None:\n",
    "            self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
    "        else:\n",
    "            self.ranking_loss = nn.SoftMarginLoss()\n",
    "\n",
    "    def forward(self, feats, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        feats: pytorch Variable, features for the targets, shape [NxD]\n",
    "        targets: pytorch Variable, target values, shape [Nx1]\n",
    "        Returns:\n",
    "        loss: pytorch Variable, with shape [1]\n",
    "        \"\"\"\n",
    "        # All pairwise distances\n",
    "        D = euclidean_distance(feats, feats)\n",
    "\n",
    "        # Compute hard distances..\n",
    "        d_ap, d_an = hard_example_mining(D, targets)\n",
    "\n",
    "        # Compute loss\n",
    "        Y = d_an.data.new().resize_as_(d_an.data).fill_(1)\n",
    "        if self.margin is not None:\n",
    "            loss = self.ranking_loss(d_an, d_ap, Y)\n",
    "        else:\n",
    "            loss = self.ranking_loss(d_an - d_ap, Y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAP2STripletLoss(nn.Module):\n",
    "#\"paper loss\"\n",
    "    def __init__(self, margin=1, coeff=10, weighting='poly'):\n",
    "        super(HAP2STripletLoss, self).__init__()\n",
    "        self.coeff = coeff\n",
    "        self.weighting = weighting\n",
    "        self.margin = margin\n",
    "        if margin is None:\n",
    "            self.ranking_loss = nn.SoftMarginLoss()\n",
    "        else:\n",
    "            self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
    "\n",
    "    def forward(self, feats, targets):\n",
    "    #\"feats embedding immagine\"        \n",
    "        # All pairwise distances\n",
    "        D = euclidean_distance(feats,feats)\n",
    "        print(D.size())\n",
    "        # Compute hard aware point to set distances..\n",
    "        d_ap, d_an = hard_aware_point_2_set_mining(D, targets, self.weighting, self.coeff)\n",
    "        d_ap.requires_grad_()\n",
    "        d_an.requires_grad_()\n",
    "\n",
    "        # Compute loss\n",
    "        Y = (d_an.data.new().resize_as_(d_an.data).fill_(1))\n",
    "        Variable(Y,requires_grad=True)\n",
    "        if self.margin is None:\n",
    "            loss = self.ranking_loss(d_an-d_ap, Y)\n",
    "        else:\n",
    "            loss = self.ranking_loss(d_an, d_ap, Y)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterLoss(nn.Module):\n",
    "    def __init__(self,alpha=0.2):\n",
    "        super(ClusterLoss,self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.ranking_loss = nn.SoftMarginLoss()\n",
    "        \n",
    "        self.clusters_sum = []\n",
    "        self.clusters_count = []\n",
    "        self.clusters_labels = []\n",
    "        \n",
    "    def forward(self,feats,targets):       \n",
    "       \n",
    "        t_intra,D_intra = self.Euclidean_intra(feats,targets)\n",
    "        t_inter,D_inter = self.Euclidean_inter(targets)\n",
    "        \n",
    "        Y = (torch.Tensor(t_intra).data.new().resize_as_(torch.Tensor(t_intra).data).fill_(1))\n",
    "        Y = Y.to(device)\n",
    "        loss = self.ranking_loss((D_intra-D_inter)+self.alpha,Y)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def mean_feats(self,feats,targets):\n",
    "        N = feats.size(0)\n",
    "        \n",
    "        # shape [N, N]\n",
    "        is_pos = targets.expand(N, N).eq(targets.expand(N, N).t())\n",
    "        is_neg = targets.expand(N, N).ne(targets.expand(N, N).t())\n",
    "        \n",
    "        target_batch = []\n",
    "        for i in range(N):\n",
    "            t = targets[i]\n",
    "            if not t in target_batch:\n",
    "                a = feats[is_pos[:,i],:]#list of features computed over the same individual\n",
    "                sum_a = torch.sum(a,dim=0)              \n",
    "\n",
    "                if self.clusters_sum:\n",
    "                    if self.clusters_labels:                       \n",
    "                        if t in self.clusters_labels:\n",
    "                            j = self.clusters_labels.index(t.item())\n",
    "                            self.clusters_sum[j] += sum_a\n",
    "                            self.clusters_count[j] += a.size(0)\n",
    "                        else:\n",
    "                            self.clusters_sum.append(sum_a)\n",
    "                            self.clusters_labels.append(t)\n",
    "                            self.clusters_count.append(a.size(0))\n",
    "                    else: \n",
    "                        print('There are no labels {}'.format(M_label))\n",
    "                else:\n",
    "\n",
    "                    self.clusters_sum.append(sum_a)\n",
    "                    self.clusters_labels.append(t.item())\n",
    "                    self.clusters_count.append(a.size(0))\n",
    "                target_batch.append(t)\n",
    "                \n",
    "    def mean_feats_compute(self):\n",
    "       \n",
    "        M_emb = [sum_a/len_a for sum_a,len_a in zip(self.clusters_sum,self.clusters_count)]\n",
    "        self.M_emb = torch.stack(M_emb).to(device)\n",
    "                \n",
    "    def Euclidean_intra(self,feats,targets):\n",
    "        \n",
    "        M_intra = self.M_emb\n",
    "        D = losses.euclidean_distance(feats,M_intra)\n",
    "        N = feats.size(0)\n",
    "                       \n",
    "        is_pos = targets.expand(N, N).eq(targets.expand(N, N).t())\n",
    "        \n",
    "        target_intra = []\n",
    "        D_intra = []\n",
    "        for i in range(N):          \n",
    "            if not targets[i].item() in target_intra:\n",
    "                D_id = D[is_pos[:,i],:]            \n",
    "                target_intra.append(targets[i].item())\n",
    "                index_mean = self.clusters_labels.index(targets[i].item())\n",
    "                D_intra.append(torch.max(D_id[:,index_mean]))\n",
    "        \n",
    "        D_intra = torch.stack(D_intra)\n",
    "        D_intra = D_intra.to(device)\n",
    "        return target_intra, D_intra\n",
    "    \n",
    "    def Euclidean_inter(self,targets):\n",
    "        M_intra = self.M_emb\n",
    "        N = targets.size(0)\n",
    "        is_neg = targets.expand(N, N).ne(targets.expand(N, N).t())\n",
    "        target_inter = []\n",
    "        D_inter= []\n",
    "        for i in range(N):\n",
    "            if not targets[i].item() in target_inter:\n",
    "                index_mean = self.clusters_labels.index(targets[i].item())\n",
    "                M = M_intra[index_mean,:]  \n",
    "                target_inter.append(targets[i].item())  \n",
    "                list_inter = []\n",
    "                for j in range(len(M_intra)):\n",
    "                    if not j == index_mean:\n",
    "                        X = (M_intra[j,:])\n",
    "                        list_inter.append(torch.pairwise_distance(M.unsqueeze(1),X.unsqueeze(1),2))\n",
    "                D_inter.append(torch.min(torch.stack(list_inter)))\n",
    "\n",
    "        \n",
    "        D_inter = torch.Tensor(D_inter)\n",
    "        D_inter = D_inter.to(device)\n",
    "        D_inter.requires_grad_()\n",
    "        return target_inter, D_inter\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
